{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 词性标注",
   "id": "91818d7b6cf54cde"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "什么是词性标注",
   "id": "d2f96b08a59e9263"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    词性：语言中对词的一种分类方法，以语法特征为主要依据、兼顾词汇意义对词进行划分的结果\n",
    "         ，常见的词性有14种，比如：名词、动词、形容词、副词等\n",
    "    顾名思义，词性标注（Part-of-speech annotation，简称POS）\n",
    "            就是标注出一段文本中 每个词汇的词性。"
   ],
   "id": "e7eaceea9f455af3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    举例：\n",
    "    我爱自然语言处理\n",
    "    ===>\n",
    "    我/rr，爱/v，自然语言/n，处理/vn\n",
    "    rr：人称名词\n",
    "    v：动词\n",
    "    n：名词\n",
    "    vn：动名词"
   ],
   "id": "b202a18edfaf55e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "词性标注的作用",
   "id": "1b6503a5656a655e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    词性标注以分词为基础，是对文本语言的另外一个角度的理解，因此也常常成为AI解决NLP领域\n",
    "    高阶任务的重要基础环节。"
   ],
   "id": "1dc54c56d0f88779"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T15:02:19.882565Z",
     "start_time": "2024-07-24T15:02:19.872095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用jieba进行中文词性标注\n",
    "import jieba.posseg as pseg\n",
    "content = \"我爱天安门\"\n",
    "result = pseg.lcut(content)\n",
    "print(result)"
   ],
   "id": "ccdbf75dd2095601",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pair('我', 'r'), pair('爱', 'v'), pair('天安门', 'ns')]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T15:07:28.373490Z",
     "start_time": "2024-07-24T15:06:51.975461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用hanlp进行中文词性标注\n",
    "import hanlp\n",
    "# 加载中文命名实体识别的预训练模型CTB5_POS_RNN_FASTTEXT_ZH\n",
    "tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)\n",
    "# 分词\n",
    "content = \"我爱天安门\"\n",
    "tokenizer = hanlp.load('CTB6_CONVSEG')\n",
    "content_lis = tokenizer(content)\n",
    "# 输入是分词结果列表\n",
    "result = tagger(content_lis)\n",
    "\n",
    "print(result)"
   ],
   "id": "62e127588c7ed21e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_fasttext_20191230_202639.zip\n",
      "If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues\n",
      "When reporting an issue, make sure to paste the FULL ERROR LOG below.\n",
      "================================ERROR LOG BEGINS================================\n",
      "OS: Windows-11-10.0.22635-SP0\n",
      "Python: 3.12.0\n",
      "PyTorch: 2.3.1+cpu\n",
      "TensorFlow: 2.17.0\n",
      "HanLP: 2.1.0-beta.58\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras._tf_keras.keras.layers' has no attribute 'AbstractRNNCell'\n=================================ERROR LOG ENDS=================================",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mhanlp\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# 加载中文命名实体识别的预训练模型CTB5_POS_RNN_FASTTEXT_ZH\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m tagger \u001B[38;5;241m=\u001B[39m \u001B[43mhanlp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhanlp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpretrained\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCTB5_POS_RNN_FASTTEXT_ZH\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# 分词\u001B[39;00m\n\u001B[0;32m      6\u001B[0m content \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m我爱天安门\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32mD:\\study\\AI\\NLP_Study\\Lib\\site-packages\\hanlp\\__init__.py:43\u001B[0m, in \u001B[0;36mload\u001B[1;34m(save_dir, verbose, **kwargs)\u001B[0m\n\u001B[0;32m     41\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhanlp_common\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconstant\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HANLP_VERBOSE\n\u001B[0;32m     42\u001B[0m     verbose \u001B[38;5;241m=\u001B[39m HANLP_VERBOSE\n\u001B[1;32m---> 43\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mload_from_meta_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43msave_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmeta.json\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\study\\AI\\NLP_Study\\Lib\\site-packages\\hanlp\\utils\\component_util.py:186\u001B[0m, in \u001B[0;36mload_from_meta_file\u001B[1;34m(save_dir, meta_filename, transform_only, verbose, **kwargs)\u001B[0m\n\u001B[0;32m    184\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m    185\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m--> 186\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m e \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\study\\AI\\NLP_Study\\Lib\\site-packages\\hanlp\\utils\\component_util.py:99\u001B[0m, in \u001B[0;36mload_from_meta_file\u001B[1;34m(save_dir, meta_filename, transform_only, verbose, **kwargs)\u001B[0m\n\u001B[0;32m     97\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mcls\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmeta_filename\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m doesn\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124mt contain classpath field\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 99\u001B[0m     obj: Component \u001B[38;5;241m=\u001B[39m \u001B[43mobject_from_classpath\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    100\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(obj, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mload\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m    101\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m transform_only:\n\u001B[0;32m    102\u001B[0m             \u001B[38;5;66;03m# noinspection PyUnresolvedReferences\u001B[39;00m\n",
      "File \u001B[1;32mD:\\study\\AI\\NLP_Study\\Lib\\site-packages\\hanlp_common\\reflection.py:27\u001B[0m, in \u001B[0;36mobject_from_classpath\u001B[1;34m(classpath, **kwargs)\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mobject_from_classpath\u001B[39m(classpath, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m---> 27\u001B[0m     classpath \u001B[38;5;241m=\u001B[39m \u001B[43mstr_to_type\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclasspath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39misfunction(classpath):\n\u001B[0;32m     29\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m classpath\n",
      "File \u001B[1;32mD:\\study\\AI\\NLP_Study\\Lib\\site-packages\\hanlp_common\\reflection.py:44\u001B[0m, in \u001B[0;36mstr_to_type\u001B[1;34m(classpath)\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"convert class path in str format to a type\u001B[39;00m\n\u001B[0;32m     35\u001B[0m \n\u001B[0;32m     36\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     41\u001B[0m \n\u001B[0;32m     42\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     43\u001B[0m module_name, class_name \u001B[38;5;241m=\u001B[39m classpath\u001B[38;5;241m.\u001B[39mrsplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 44\u001B[0m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m)\u001B[49m, class_name)\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\n",
      "File \u001B[1;32mC:\\Program Files (x86)\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m     88\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m     89\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m---> 90\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1381\u001B[0m, in \u001B[0;36m_gcd_import\u001B[1;34m(name, package, level)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1354\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1325\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:929\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:994\u001B[0m, in \u001B[0;36mexec_module\u001B[1;34m(self, module)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:488\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[1;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[1;32mD:\\study\\AI\\NLP_Study\\Lib\\site-packages\\hanlp\\components\\taggers\\pos_tf.py:4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# -*- coding:utf-8 -*-\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# Author: hankcs\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Date: 2019-12-05 23:05\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhanlp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomponents\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtaggers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcnn_tagger_tf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CNNTaggerTF\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhanlp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomponents\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtaggers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrnn_tagger_tf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RNNTaggerTF\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mCNNPartOfSpeechTaggerTF\u001B[39;00m(CNNTaggerTF):\n",
      "File \u001B[1;32mD:\\study\\AI\\NLP_Study\\Lib\\site-packages\\hanlp\\components\\taggers\\cnn_tagger_tf.py:9\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Union, Tuple, Any, List, Iterable\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhanlp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomponents\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtaggers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtagger_tf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TaggerComponent\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhanlp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransform\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtsv_tf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TSVTaggingTransform\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhanlp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcommon\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvocab_tf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m VocabTF\n",
      "File \u001B[1;32mD:\\study\\AI\\NLP_Study\\Lib\\site-packages\\hanlp\\components\\taggers\\tagger_tf.py:10\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhanlp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcommon\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras_component\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KerasComponent\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhanlp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcrf\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcrf_layer_tf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CRF, CRFLoss, CRFWrapper\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhanlp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchunking\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01miobes_tf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m IOBES_F1_TF\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mTaggerComponent\u001B[39;00m(KerasComponent, ABC):\n",
      "File \u001B[1;32mD:\\study\\AI\\NLP_Study\\Lib\\site-packages\\hanlp\\layers\\crf\\crf_layer_tf.py:18\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# ******************************************************************************\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# Copyright 2017-2018 Intel Corporation\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# ******************************************************************************\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhanlp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcrf\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcrf_tf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m crf_decode, crf_log_likelihood\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mCRF\u001B[39;00m(tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mLayer):\n\u001B[0;32m     22\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Conditional Random Field layer (tf.keras)\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;124;03m    `CRF` can be used as the last layer in a network (as a classifier). Input shape (features)\u001B[39;00m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;124;03m    must be equal to the number of classes the CRF can predict (a linear layer is recommended).\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     43\u001B[0m \n\u001B[0;32m     44\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
      "File \u001B[1;32mD:\\study\\AI\\NLP_Study\\Lib\\site-packages\\hanlp\\layers\\crf\\crf_tf.py:366\u001B[0m\n\u001B[0;32m    362\u001B[0m     viterbi_score \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmax(trellis[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m    363\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m viterbi, viterbi_score\n\u001B[1;32m--> 366\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mCrfDecodeForwardRnnCell\u001B[39;00m(\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAbstractRNNCell\u001B[49m):\n\u001B[0;32m    367\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Computes the forward decoding in a linear-chain CRF.\"\"\"\u001B[39;00m\n\u001B[0;32m    369\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, transition_params, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'keras._tf_keras.keras.layers' has no attribute 'AbstractRNNCell'\n=================================ERROR LOG ENDS================================="
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ef78533b4bb8a772"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
