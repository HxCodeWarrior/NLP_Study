{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 自动求导",
   "id": "4297444d29740bf9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    PyTorch中，所有神经网络的核心是autograd包。autograd包为张量上的所有操作提供了自\n",
    "    动求导机制。它是一个在运行时定义（define-by-run）的框架，这意味着反向传播是根据代\n",
    "    码如何运行来决定的，并且每次迭代可以是不同的。"
   ],
   "id": "6ed61423cd58d9d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    本节学习：Autograd的求导机制\n",
    "            梯度的反向传播"
   ],
   "id": "53c6a97ad2a88d61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    torch.Tensor是这个包的核心类。如果设置了它的属性 .requires_grad为True，那么它\n",
    "    将会追踪对于该张量的所有操作。当计算完成后可以通过调用 .backward()，来自动计算所\n",
    "    有的梯度。这个张量的所有梯度会自动累加到 .grad属性。"
   ],
   "id": "149f742e14b9bd4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    注意：在y.backward（）时，如果y是标量，则不需要为backward（）传入任何参数；否则\n",
    "         需要传入一个与y同形的Tensor。"
   ],
   "id": "18e248635fdb2782"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    要阻止一个张量被跟踪历史，可以调用 .detach()方法将其与计算历史分离，并阻止它未来的\n",
    "    计算记录被跟踪。为了防止跟踪历史记录（和使用内存），可以将代码块包装在with \n",
    "    torch.no_grad（）：中。在评估模型时特别有用，因为模型可能具有requires_grad = \n",
    "    True 的可训练的参数，但是我们不需要在此过程中对他们进行梯度计算。"
   ],
   "id": "90f5b37c2099737c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    还有一个类对于autograd的实现非常重要：Function。Tensor和Function互相连接生成\n",
    "    了一个无环图（acylic graph），它编码了完整的计算历史。每个张量都有一个 .grad_fn\n",
    "    属性，该属性引用了创建Tensor自身的Funtion（除非这个张量使用户手动创建的，即这个张\n",
    "    量的grad_fn是None）。下面给出的例子中，张量由用户手动创建，因此grad_fn返回结果为None"
   ],
   "id": "1db877a582ba9dd2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T14:43:41.165970Z",
     "start_time": "2024-07-22T14:43:40.091572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "x = torch.randn(3, 3, requires_grad=True)\n",
    "print(x.grad_fn)"
   ],
   "id": "d5c33e7d8725976b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    如果需要计算导数，可以在Tensor上调用 .backward（）。如果Tensor是一个标量（即它\n",
    "    包含一个元素的数据），则不需要为backward（）指定任何参数，但是如果它有更多的元素，\n",
    "    则需要指定一个gradient参数，该参数是形状匹配的张量。"
   ],
   "id": "f4392dbdb0e9fd0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T14:54:02.439078Z",
     "start_time": "2024-07-22T14:54:02.431024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 创建一个张量，并设置requires-grad = True用来追踪其计算历史\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "# 对这个张量做一次运算\n",
    "y = x ** 2\n",
    "print(y)\n",
    "# y是计算的结果，所以它有grad_fn属性\n",
    "print(y.grad_fn)\n",
    "# 对y进行更多的操作\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z)\n",
    "print(out)"
   ],
   "id": "a774b3076d71ec6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], grad_fn=<PowBackward0>)\n",
      "<PowBackward0 object at 0x00000259BC8971F0>\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<MulBackward0>)\n",
      "tensor(3., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    .requires_grad_(...) 原地改变了现有张量的requires_grad标志。如果没有指定的话\n",
    "    ，默认输入的这个标志是 False。"
   ],
   "id": "1d2fdc2ea50fb111"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
